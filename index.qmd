---
title: "A Unified Probabilistic Framework for Non-Stationary Heart Rate Variability Analysis"
subtitle: "Modeling the Dynamic Evolution of Autonomic Control"
date-modified: today
bibliography: "bibliography.bib"
execute: 
  echo: false
author: 
  - name: "Matías Castillo-Aguilar"
    email: matias.castillo@umag.cl
    orcid: 0000-0001-7291-247X
  - name: "Cristian Núñez-Espinosa"
    email: cristian.nunez@umag.cl
    orcid: 0000-0002-9896-7062
format: 
  html: 
    fig-format: svg
    title-block-banner: true
    title-block-style: manuscript
    toc: true
    toc-location: right
  pdf: 
    fig-format: pdf
    fig-dpi: 500
    documentclass: article
fig-width: 8
fig-height: 6
editor_options: 
  chunk_output_type: console
---

# Introduction

The rhythm of the human heart, far from being a simple, constant beat, is a complex and dynamic signal reflecting the continuous interplay between an organism and its internal and external environments [@arakaki2023connection]. The precise timing between successive heartbeats, measured as the R-R interval (RRi), is a primary non-invasive proxy for autonomic nervous system (ANS) activity. The analysis of variations in this interval, a biomarker known as heart rate variability (HRV), has become a cornerstone for assessing cardiovascular health, stress responses, and overall physiological state [@arakaki2023connection; @huber2025brain]. This variability arises from the coordinated actions of multiple regulatory systems, including the sympathetic and parasympathetic branches of the ANS, baroreflexes, and thermoregulatory mechanisms [@tiwari2021analysis].

Despite its utility, a significant limitation of traditional HRV analysis is its reliance on time-domain or frequency-domain metrics derived from static, short-term data windows [@kim2021ultra; @lu2023uncertainties]. The conventional approach, standardized by consensus, typically involves calculating metrics over a five-minute recording period. In the time domain, this yields summary statistics such as the standard deviation of normal-to-normal intervals (SDNN) or the root mean square of successive differences (RMSSD) [@shaffer2020critical]. While informative, these metrics fundamentally collapse a dynamic, evolving process into a single, static number [@kim2021ultra]. An RMSSD value of 40 ms, for example, provides an aggregate measure of high-frequency variability but offers no information about whether that variability was constant, increasing, or decreasing over the five-minute window. This temporal averaging obscures the very dynamics that are often of greatest physiological interest, particularly when an organism is adapting to a challenge [@kim2021ultra]. For a system in constant flux, a single mean or standard deviation is an impoverished descriptor, akin to summarizing a film with a single still frame.

This issue is arguably more acute in conventional frequency-domain analysis. Methods based on the Fast Fourier Transform (FFT) are predicated on the mathematical assumption that the signal is wide-sense stationary—that its statistical properties, such as mean and variance, are constant over the analysis window. Yet, physiological signals, especially during transitions like the onset of exercise or recovery from a cognitive stressor, are the epitome of non-stationarity. When a non-stationary signal is subjected to an FFT, the resulting power spectrum is not a true representation of any single physiological state. Instead, it becomes a composite, a smeared average of all the different spectral states that occurred within the window. A peak in the low-frequency (LF) band, for instance, might not reflect sustained sympathetic activity but rather the aliased result of a rapid change in heart rate. Consequently, while such methods may offer insights into a snapshot of a physiological state under controlled, steady-state conditions, they are fundamentally ill-suited to capture the dynamic, non-stationary nature of RRi signals during physiological transitions or stress [@tiwari2021analysis; @lu2023uncertainties]. The implicit assumption of stationarity in these fixed-window approaches can obscure subtle, yet physiologically critical, shifts in heart rate and its underlying variability [@kim2021ultra; @lu2023uncertainties]. This methodological constraint forces researchers into a difficult compromise: use short windows to better approximate stationarity at the cost of poor frequency resolution, or use long windows to achieve better frequency resolution at the cost of violating the stationarity assumption more severely. Furthermore, these conventional analyses often fail to establish a direct mechanistic link between observed changes in HRV and the underlying physiological processes responsible for them. A change in a summary statistic does not, by itself, constitute a model of the physiological process that produced it; it is a description of an outcome, not an explanation of a mechanism.

While various advanced modeling approaches have attempted to address these limitations, they often fall short in other critical areas, leaving a significant gap in the analytical toolkit. For instance, state-space models have shown promise in tracking the evolution of cardiac dynamics [@rosas2024bayesian; @liu2024efficient], conceptualizing the RRi signal as an observation of a hidden, underlying physiological state that evolves through time. This approach is conceptually elegant and explicitly built to handle non-stationarity. However, its practical application is frequently hindered by significant challenges. The complexity of defining a physiologically realistic state transition model can lead to models with a large number of parameters, making robust estimation difficult and increasing the risk of overfitting. Furthermore, the inferred hidden states and transition matrices often lack a direct, one-to-one correspondence with clear physiological concepts. A latent state variable may track changes in variability, but it does not inherently distinguish between, for example, parasympathetic withdrawal and sympathetic activation, thereby limiting the model's physiological interpretability.

On another front, time-frequency analysis methods have become a popular tool for visualizing non-stationary signals. Techniques like the Short-Time Fourier Transform (STFT) or the Continuous Wavelet Transform (CWT) produce intuitive time-frequency representations, or spectrograms, that show how spectral power changes over time. The CWT, in particular, offers advantages through its multi-resolution analysis, providing better temporal resolution for high-frequency events and better frequency resolution for low-frequency events. These methods can successfully visualize the time-varying nature of spectral content, revealing, for example, a decrease in high-frequency power following a stimulus. However, their primary limitation is that they are fundamentally descriptive, not generative. They provide a processed depiction of the data but do not constitute a formal model of the underlying process. As such, they cannot be used to test specific physiological hypotheses in a principled way. From a wavelet spectrogram, one can observe a change, but one cannot estimate a parameter for the *rate* of that change, the *completeness* of recovery, or the *statistical significance* of a temporal shift. Consequently, no existing model provides a unified, probabilistic framework that can simultaneously capture and mechanistically interpret changes in both the mean RRi and its multi-component variability. The field is thus caught between overly simplistic windowed methods that discard dynamic information and more sophisticated techniques that are either difficult to interpret or are not structured for formal, mechanistic hypothesis testing.

To advance the field beyond these constraints, a new generation of statistical models is clearly needed. A framework capable of surpassing these barriers should, at a minimum, satisfy four critical criteria. First, it must explicitly address the non-stationarity inherent to physiological signals, obviating the need for arbitrary analysis windows. Second, it must be capable of mechanistically decomposing the signal into its distinct components: a gross, underlying trend in heart rate and the structured, oscillatory variability that represents physiological regulation. Third, its parameters should have a direct, interpretable link to specific physiological processes, such as autonomic tone and recovery dynamics. Finally, as a probabilistic framework, it must provide a principled means of quantifying uncertainty for all estimates, moving beyond a reliance on simple point estimates.

To meet these challenges, we therefore propose a novel probabilistic framework. Here, we introduce a model designed to directly confront non-stationarity by treating the RRi signal as a continuous stochastic process. This framework is built to mechanistically decompose the signal into its constituent parts, including a baseline trend and its structured variability. Its core components are defined by flexible functions whose parameters are intended to provide direct physiological insight into response and recovery dynamics. Finally, we will demonstrate how its implementation as a fully Bayesian model yields the robust uncertainty quantification required of a modern analytical framework.

This study introduces and validates a novel probabilistic model for analyzing non-stationary RRi signals. We test the central hypothesis that this framework provides a more robust, informative, and physiologically interpretable analysis of heart rate dynamics than traditional methods. Specifically, we demonstrate its ability to accurately capture transient perturbations and show that its parameters offer enhanced mechanistic nuance into autonomic control. Through this work, we seek to establish a new standard for the analysis of dynamic physiological signals.

```{r}
#| include: false
library(ggplot2)
library(data.table)
library(brms)
library(tidybayes)
library(rstan)

theme_set(
  new = theme_classic(base_size = 14) +
    theme(legend.position = "bottom")
)
```

# Results

This section presents the performance and application of our novel generative model for RRi analysis. We first establish the model's validity and accuracy through a series of simulation studies, demonstrating its ability to recover known ground-truth parameters. We then provide a direct quantitative comparison against conventional windowed analysis techniques, highlighting the superior temporal and spectral resolution of our approach. Finally, we apply the model to an empirical dataset from a physiological perturbation protocol to illustrate its capacity for yielding enhanced mechanistic insights into autonomic dynamics.

## Model validation and parameter recovery on synthetic data

To verify the model's correctness and inferential accuracy, we first applied it to synthetic RRi data generated with known underlying parameters. This allowed for a direct comparison between the model's estimates and the ground-truth dynamics, providing a gold-standard validation of its performance. These separate scenarios can be seen in Figure 1 and the parameters used to generate the data can be seen in Table XX.

::: {.content-visible when-format="html"}
![](figures/fig-generated-data.svg){width=100%}
:::
::: {.content-visible when-format="pdf"}
![](figures/fig-generated-data.pdf){width=100%}
:::
**Figure XX**. Three scenarios simulated from the probabilistic model to assess the inference capabilities to recover the data generation process. The scenarios correspond to (A) a classic sympatho-vagal response, (B) an incomplete recovery with spectral persistence, and (C) a high noise with a stable spectrum. Subpanels indicate observed RRi signal with true component ($\mu(t_i)$) highlighted, true underlying RRi trend and SDNN, next to spectral signature of the signal, depicting the relative contribution of each frequency band as a dynamic process.

|Parameter    |Interpretation                           |Scenario A      |Scenario B      |Scenario C        |
|:------------|:----------------------------------------|:---------------|:---------------|:-----------------|
|$\tau$       |Time onset                               |6.00            |6.00            |6.00              |
|$\delta$     |Stimulus durantion                       |3.00            |3.00            |3.00              |
|$\lambda$    |Rate of dynamics onset                   |3.00            |3.00            |1.50              |
|$\phi$       |Rate of recovery                         |2.00            |2.00            |1.50              |
|$\alpha_r$   |Resting RRi                              |800.00          |950.00          |900.00            |
|$\beta_r$    |Magnitude of RRi decay                   |300.00          |450.00          |300.00            |
|$c_r$        |RRi recovery proportion                  |1.00            |0.50            |1.00              |
|$\alpha_s$   |Resting SDNN                             |60.00           |60.00           |100.00            |
|$\beta_s$    |Magnitude of SDNN decay                  |40.00           |40.00           |20.00             |
|$c_s$        |SDNN recovery proportion                 |1.00            |0.60            |1.00              |
|$\pi_{base}$ |Base-state [VLF, LF, HF] proportion      |[0.2, 0.4, 0.4] |[0.1, 0.2, 0.7] |[0.1, 0.45, 0.45] |
|$\pi_{pert}$ |Perturbed-state [VLF, LF, HF] proportion |[0.7, 0.2, 0.1] |[0.7, 0.2, 0.1] |[0.2, 0.5, 0.3]   |
|$c_c$        |Spectral recovery proportion             |0.80            |0.40            |1.00              |
|$w$          |Proportion of structured variance        |0.90            |0.90            |0.60              |
**Table 2**. Ground-truth parameter values used in the generation of three simulation scenarios. The scenarios correspond to (A) a classic sympatho-vagal response, (B) an incomplete recovery with spectral persistence, and (C) a high noise with a stable spectrum.

Our model accurately reconstructs the full, continuous dynamics of the RRi signal. Figure 2 showcases the model's performance on a representative simulation of a classic sympatho-vagal response. The top panels display the model's posterior mean estimates for the baseline RRi trajectory ($\mathrm{RR_{base}}(t)$) and the total variability trajectory ($\mathrm{SDNN}(t)$). The estimates closely follow the true generative trajectories, and the 95% posterior credible intervals consistently envelop the ground truth, demonstrating both accuracy and uncertainty quantification. The bottom panel shows the corresponding reconstruction of the time-varying spectral proportions ($p_j(t)$), where the model again successfully captures the rapid, dynamic shift from a HF dominant state at baseline to a VLF dominant state during the perturbation.

::: {.content-visible when-format="html"}
![](figures/fig-model-method.svg){width=100%}
:::
::: {.content-visible when-format="pdf"}
![](figures/fig-model-method.pdf){width=100%}
:::
**Figure 2**. Reconstruction of synthetic data from model estimates. The model's posterior estimates (solid lines) and 95% credible intervals (shaded ribbons) for the baseline RRi, total SDNN, and spectral proportions trajectories are shown overlaid on the known ground-truth dynamics (dashed lines). The model demonstrates high fidelity in capturing both time-domain and frequency-domain non-stationarities.

Beyond reconstructing the continuous trajectories, the model also accurately recovers the underlying mechanistic parameters that control them. Table 2 summarizes the posterior estimates for key generative parameters across all three challenging simulation scenarios. For critical parameters such as the timing of the response ($\tau$), the recovery delay ($\delta$), the magnitudes of the RRi and SDNN drops ($\beta_r$, $\beta_s$), and the fraction of structured variance ($w$), the 95% credible intervals consistently contained the true values used for data generation. This robust performance across diverse conditions confirms the model's ability to reliably estimate the interpretable parameters at the core of its design.

|Parameter      |Truth | Estimate|95% CI           |Truth | Estimate|95% CI           |Truth | Estimate|95% CI           |
|:-------|:-----|--------:|:------------------|:-----|--------:|:------------------|:-----|--------:|:------------------|
|$\lambda$         |3.0  |     2.76|[2.36, 3.19]     |3.0  |     2.80|[2.41, 3.22]     |1.5  |     1.42|[1.17, 1.71]     |
|$\phi$            |2.0  |     1.98|[1.71, 2.26]     |2.0  |     2.00|[1.66, 2.36]     |1.5  |     1.49|[1.22, 1.78]     |
|$\tau$            |6.0  |     5.99|[5.9, 6.09]      |6.0  |     6.01|[5.92, 6.09]     |6.0  |     6.02|[5.76, 6.27]     |
|$\delta$          |3.0  |     2.99|[2.77, 3.19]     |3.0  |     2.93|[2.67, 3.19]     |3.0  |     2.97|[2.54, 3.41]     |
|$\alpha_r$        |950  |   949.47|[947.66, 951.19] |950  |   949.21|[947.47, 950.97] |900  |   897.78|[890.86, 904.77] |
|$\beta_r$         |250  |   252.06|[237.16, 267.91] |250  |   253.04|[241.95, 266.19] |300  |   303.85|[266.17, 347.8]  |
|$c_r$             |1.0  |     1.00|[0.99, 1.01]     |0.5  |     0.51|[0.48, 0.53]     |1.0  |     1.01|[0.98, 1.04]     |
|$\alpha_s$        |60   |    60.76|[59.13, 62.45]   |60   |    60.72|[59.12, 62.38]   |100  |   100.35|[95.85, 104.53]  |
|$\beta_s$         |40   |    40.50|[36.77, 44.31]   |40   |    40.89|[37.47, 44.59]   |20   |    19.75|[9.19, 30.25]    |
|$c_s$             |1.0  |     0.93|[0.87, 0.99]     |0.6  |     0.54|[0.48, 0.59]     |1.0  |     0.77|[0.44, 1.07]     |
|$c_c$             |0.8  |     0.84|[0.79, 0.88]     |0.4  |     0.47|[0.37, 0.57]     |1.0  |     0.93|[0.81, 0.99]     |
|$b$               |1.0  |     1.02|[0.76, 1.3]      |1.0  |     1.09|[0.83, 1.35]     |1.0  |     0.93|[0.66, 1.21]     |
|$w$               |0.9  |     0.90|[0.89, 0.91]     |0.9  |     0.90|[0.89, 0.91]     |0.6  |     0.60|[0.57, 0.62]     |
|$\pi_{base}$[VLF] |0.1  |     0.10|[0.07, 0.14]     |0.1  |     0.09|[0.06, 0.12]     |0.1  |     0.10|[0.07, 0.15]     |
|$\pi_{base}$[LF]  |0.2  |     0.20|[0.16, 0.25]     |0.2  |     0.19|[0.15, 0.24]     |0.45 |     0.46|[0.39, 0.53]     |
|$\pi_{base}$[HF]  |0.7  |     0.70|[0.63, 0.77]     |0.7  |     0.72|[0.66, 0.78]     |0.45 |     0.44|[0.36, 0.52]     |
|$\pi_{pert}$[VLF] |0.5  |     0.51|[0.4, 0.62]      |0.7  |     0.70|[0.59, 0.8]      |0.2  |     0.23|[0.14, 0.33]     |
|$\pi_{pert}$[LF]  |0.3  |     0.28|[0.22, 0.35]     |0.2  |     0.19|[0.13, 0.25]     |0.5  |     0.47|[0.37, 0.57]     |
|$\pi_{pert}$[HF]  |0.2  |     0.20|[0.11, 0.3]      |0.1  |     0.10|[0.03, 0.19]     |0.3  |     0.30|[0.18, 0.42]     |

**Table 2**.

## Superiority of the generative model over conventional windowed methods

Having established the model's validity, we next benchmarked its performance directly against traditional analysis techniques, specifically, sliding-window analysis for time-domain metrics and the Short-Time Fourier Transform (STFT) for spectral dynamics.

The model provides superior temporal resolution for time-domain dynamics. Conventional sliding-window methods inherently smooth and delay the detection of dynamic changes. Figure 3 illustrates this critical limitation by comparing the model's instantaneous $\mathrm{SDNN}(t)$ estimate against that of a 60-second sliding window. While our model's estimate precisely tracks the non-linear recovery phase of the ground-truth signal, the windowed approach produces a delayed, blunted, and stepwise approximation that obscures the true physiological dynamics. This demonstrates the model's ability to overcome the core limitation of stationarity assumptions that plagues traditional methods.

::: {.content-visible when-format="html"}
![](figures/fig-windowed-method.svg){width=100%}
:::
::: {.content-visible when-format="pdf"}
![](figures/fig-windowed-method.pdf){width=100%}
:::
**Figure 3**. Reconstruction of synthetic data from windowed methods. The moving estimates from windowed methods (solid lines) for the baseline RRi, total SDNN, and spectral proportions trajectories are shown overlaid on the known ground-truth dynamics (dashed lines). The windowed methods introduces significant lag and its not able to separate propperly the noise from the signal, introducing biased estimators.

The model resolves spectral dynamics without the time-frequency uncertainty trade-off. The STFT is fundamentally limited by the trade-off between temporal and frequency resolution. Our model provides a continuous, and accurate depiction of the switch from HF to VLF dominance. In contrast, the STFT spectrogram produces a smeared and indistinct representation, failing to precisely resolve either the timing or the magnitude of the spectral shift.

Quantitative analysis confirms the model's superior accuracy. The visual advantages of our approach are supported by quantitative performance metrics. Table 3 presents the Root Mean Squared Error (RMSE) and coefficient of determination ($R^2$) for our model versus conventional methods across all simulation scenarios. For both time-domain and spectral-domain reconstructions, our model consistently achieved a significantly lower RMSE and a higher $R^2$, often by a substantial margin, particularly in scenarios involving rapid transitions or high noise. These results provide definitive evidence that the model's window-free, generative approach yields a more accurate and faithful representation of heart rate dynamics.

<<Tabla 3>>

## Application to an empirical dataset

To demonstrate the model's utility in a real-world context, we applied it to an RRi series recorded from a healthy subject undergoing a cold pressor test, a potent sympatho-excitatory stimulus. This allows us to move beyond validation and showcase the model's power for physiological discovery.

The model mechanistically decomposes the full autonomic response. Figure 4 presents the complete decomposition of the empirical RRi signal. The model reveals a multi-phasic response not easily captured by traditional metrics. Panel A shows the raw RRi data. Panel B illustrates the inferred $\mathrm{RR_{base}}(t)$ trajectory, quantifying a rapid initial tachycardia followed by a partial, slow recovery. Panel C reveals the dynamics of total variability ($\mathrm{SDNN}(t)$), which shows a profound suppression that persists long after the mean RRi has begun to recover. Most critically, Panel D uncovers the underlying spectral shifts: the response is characterized by an almost complete withdrawal of parasympathetically-mediated HF power and a corresponding surge in sympathetically-mediated VLF power.


**Figure 4**. Mechanistic Decomposition of an Empirical Cold Pressor Response. The model's analysis of a real RRi signal. (A) Raw data. (B) Inferred baseline RRi trajectory. (C) Inferred total SDNN trajectory. (D) Inferred spectral proportion dynamics, showing a clear shift from HF to VLF dominance. Shaded regions are 95% credible intervals.

Model parameters provide novel, interpretable insights into recovery dynamics. The true power of the model lies in the physiological interpretation of its parameters. Figure 5 displays the posterior distributions for key recovery parameters. We found that the recovery of the mean RRi ($c_r$) was significantly more complete than the recovery of total HRV ($c_s$), suggesting a lingering sympathetic activation even after the heart rate has partially normalized. Furthermore, the spectral recovery parameter ($c_c$) was near zero, indicating a profound "spectral persistence" where the VLF-dominant state remained long after the stimulus ended. Finally, the structured variance fraction ($w$) was estimated to be high throughout, confirming that the observed variability was dominated by structured autonomic oscillations rather than noise. These parameters provide a nuanced, quantitative, and mechanistic narrative of the physiological response that is inaccessible with conventional analysis.

<<Figura 5>>

# Discussion

The analysis of heart rate variability has long served as an indispensable tool for probing the complexities of the autonomic nervous system. However, the field has been persistently challenged by the non-stationary nature of physiological signals, a characteristic that fundamentally conflicts with the stationarity assumptions underpinning traditional analysis methods. The present study was designed to address these limitations by developing and validating a novel, fully generative probabilistic model for R-R interval time series. The results of our simulation studies and empirical application confirm our central hypothesis: the proposed framework provides a more robust, informative, and physiologically interpretable analysis of heart rate dynamics than conventional, windowed techniques. This discussion will interpret the significance of these findings, place them within the broader context of physiological modeling, acknowledge the model's limitations, and outline promising directions for future research.

The primary contribution of this work is the demonstration of a model that successfully overcomes the principal obstacles in dynamic HRV analysis. The results from our simulation studies provide robust evidence for the model's internal validity and inferential accuracy. Across multiple, challenging scenarios with known ground-truth dynamics, the model consistently recovered the underlying generative parameters with high precision, as evidenced by the posterior credible intervals that reliably contained the true values. Furthermore, the model accurately reconstructed the continuous, non-linear trajectories of both the mean RRi and its time-varying standard deviation, showcasing its ability to faithfully represent the signal's evolution. This performance establishes a crucial foundation of trust in the model's mechanics and its capacity to make accurate inferences from complex data.

Building upon this validation, the direct benchmarking against conventional methods highlighted the practical advantages of our approach. The quantitative analyses, which demonstrated consistently lower Root Mean Squared Error (RMSE) and higher coefficients of determination ($R^2$) for our model, provide objective evidence of its superior accuracy in tracking dynamic signals. This superiority stems directly from the model's architecture, which meets the four critical criteria outlined in the introduction. First, it explicitly models non-stationarity, obviating the need for arbitrary analysis windows. Second, it performs a mechanistic decomposition of the signal into a baseline trend, structured variability, and residual noise. Third, its parameters are designed for direct physiological interpretation. Finally, its implementation within a Bayesian framework provides principled and robust uncertainty quantification for all estimates. The successful fulfillment of these criteria represents a significant step towards a more powerful and nuanced analysis of physiological signals.

## Methodological superiority and the abandonment of stationarity

A central argument of this paper is that the limitations of traditional HRV analysis are not merely technical but are conceptual, rooted in the flawed assumption of signal stationarity. Our results provide a clear illustration of why this assumption is so problematic. Windowed methods, whether for time-domain or frequency-domain analysis, are fundamentally signal processing filters. A sliding-window calculation of the mean or standard deviation, for instance, acts as a low-pass filter on the true underlying dynamic trajectory. This process inherently introduces artifacts: it blunts the magnitude of rapid changes, creates artificial delays in their detection, and produces a stepwise output that misrepresents the smooth nature of physiological regulation. The comparison in our results, where the windowed SDNN estimate lagged behind and smoothed over the true recovery dynamic, provides a stark example of this distortion.

A similar and well-documented issue plagues windowed spectral analysis, such as the Short-Time Fourier Transform (STFT). The STFT is constrained by the Heisenberg-Gabor uncertainty principle, which dictates an inescapable trade-off between temporal and frequency resolution. Short analysis windows can pinpoint the timing of spectral changes but produce noisy and poorly resolved frequency estimates; long windows provide cleaner spectral estimates but cannot precisely locate them in time. This limitation was evident in our benchmarking results, where the STFT produced a smeared and ambiguous representation of the rapid sympatho-vagal shift that our model resolved with high precision.

Our model circumvents these issues by reframing the problem from one of signal processing to one of statistical inference. Instead of filtering the data to reveal its properties, the model posits a generative process with an explicit mathematical structure, the double-logistic function, and infers the parameters of that process from the data. Because the model has a formal representation for the entire time-course of the physiological response, it is not constrained by the localizing limitations of a window. It uses information from the entire time series to inform its estimate at any single point, allowing it to simultaneously achieve high resolution in both the time and frequency domains. This fundamental shift from a descriptive, filtering approach to a generative, inferential one is the core reason for its demonstrated methodological superiority.

## Physiological and clinical significance of the model's parameters

The true value of this modeling framework lies not only in its improved accuracy but in its ability to yield deeper physiological insight. Traditional HRV metrics provide descriptive summaries of the signal, whereas the model's parameters offer a more mechanistic, quantitative narrative of the underlying autonomic processes. This transition from description to mechanism has significant implications for both basic physiological research and potential clinical applications.

A key area of innovation is the model's detailed quantification of recovery dynamics. Standard analysis might report that HRV returned to baseline after a certain time, but our model deconstructs this recovery into distinct, interpretable components. The parameters for recovery rate ($\phi$), delay ($\delta$), and completeness ($c_s$, $c_c$) allow for a multi-dimensional assessment of autonomic resilience. For example, the model could distinguish between an individual with a rapid but incomplete recovery and one with a slow but complete recovery, a nuance entirely lost to traditional methods. The ability to separately quantify the recovery completeness of the mean R-R interval ($c_r$), total variability ($c_s$), and spectral balance ($c_c$) could be particularly powerful. A finding of dissonant recovery, for instance, a rapid return of heart rate to baseline but a persistently suppressed HRV, could serve as a sensitive marker for lingering sympathetic activation or impaired parasympathetic reactivation, concepts central to stress physiology and cardiovascular health.

Furthermore, the introduction of the structured variance fraction, $w$, opens a new avenue for interpreting the *nature* of heart rate variability. This parameter quantifies the balance between coherent, physiologically structured oscillations and unstructured, residual noise. A high value of $w$ would suggest a state where the majority of variability is driven by well-organized autonomic regulation, whereas a low value could indicate a breakdown of this regulation, a pathological state, or a high degree of measurement noise. This parameter may therefore provide a novel index of autonomic coherence. For instance, in clinical contexts, a chronically low $w$ could be a marker for conditions like atrial fibrillation or other dysautonomias where the RRi signal becomes more erratic. In sports science, tracking $w$ could help distinguish between the healthy, structured variability of a well-recovered athlete and the potentially noisy, less structured variability of an over-trained one. These novel parameters, taken together, provide a richer and more granular toolkit for interrogating physiological states.

## Limitations and considerations for future work

Despite its advantages, the proposed model is not without limitations, and an honest appraisal of these is essential for guiding future research. The first and most practical limitation of this model is its computational demand, which scales with the time series length ($N$) and spectral resolution ($N_{sin}$) at a rate of $O(N \cdot N_{sin} + N_{sin}^2)$. This scaling can pose a substantial barrier to the analysis of high-dimensional physiological data. For instance, applying the model to long-duration recordings, such as 24-hour Holter monitor data, or to studies requiring fine-grained spectral detail (a large $N_{sin}$), could result in prohibitively long runtimes. This computational burden not only constrains the model's feasibility for large-scale clinical studies but also hinders the iterative process of model development and validation, where rapid feedback is essential for scientific progress.

Second, the model's architecture relies on specific structural assumptions. The choice of the double-logistic function to represent the perturbation-recovery dynamic is central to the model's interpretability. While this function is highly flexible and well-suited for the acute, transient stressors investigated here, it is by no means a universal model for all physiological dynamics. More complex, multi-phasic responses or long-term phenomena like circadian rhythms would not be well-captured by this specific functional form. Similarly, the spectral component of the model assumes that the signal can be effectively represented by a sum of sinusoids within pre-defined frequency bands. While this is a standard and powerful approach, it remains a model of the underlying spectral density and is constrained by the choice of those fixed frequencies.

Finally, the scope of the model's validation must be considered. The current study demonstrated the model's efficacy on simulated data and provided a proof-of-concept application on a single, acute physiological stressor. Its generalizability to other contexts has not yet been established. The model's performance on different types of physiological challenges, such as orthostatic stress, cognitive load, or pharmacological interventions, requires explicit investigation. Moreover, its application to long-term, 24-hour ambulatory recordings presents both a significant opportunity and a challenge, as such data often contain multiple, overlapping events and distinct physiological states (e.g., wakefulness, sleep) that would require extensions to the current model structure.

## Future directions

The limitations of the current model naturally illuminate several promising avenues for future research. To address the issue of fixed functional forms, the double-logistic component could be replaced with more flexible, non-parametric alternatives, such as Gaussian processes or penalized splines. This would create a semi-parametric model capable of capturing a much wider array of dynamic shapes, albeit potentially at the cost of some of the direct parameter interpretability. To enhance the model's explanatory power and move towards subject-level inference, it could be extended into a hierarchical or mixed-effects framework. In such a model, key parameters like recovery rates or response magnitudes could be modeled as functions of subject-level covariates, such as age, sex, disease status, or fitness level, allowing for formal statistical testing of group differences in autonomic dynamics.

A clear priority for future work is the broad application and validation of the model across diverse datasets. Applying the model to existing clinical cohorts, for example, in studies of post-myocardial infarction risk stratification or diabetic autonomic neuropathy, could reveal whether its novel parameters offer superior prognostic value compared to traditional HRV metrics. Furthermore, deploying the model to analyze data from controlled interventions, such as exercise training programs or mindfulness-based stress reduction, would allow for a mechanistic quantification of how these interventions reshape autonomic function. On the computational front, the development of efficient approximate inference techniques, such as variational Bayes, could drastically reduce runtime and make the model a more practical tool for large-scale research and, eventually, clinical use.

# Methods

## Model Formulation

We presented a comprehensive generative model for the RRi signal, observed at a discrete set of time points $\{t_i\}_{i=1}^N$. The model's architecture is designed to deconstruct the signal into its constituent physiological components, providing a mechanistic account of the processes that shape heart rate dynamics. This is achieved by conceptualizing the signal as a probabilistic process controlled by a time-varying mean, $\mu(t_i)$, and a partitioned, time-varying standard deviation. This sophisticated approach moves beyond simple curve-fitting to formalize a theory of autonomic control and its temporal evolution within a fully Bayesian framework.

The cornerstone of the model is the partitioning of the total signal variance into two distinct, time-dependent components: a structured variance, $\sigma^2_{\text{struct}}(t_i)$, which captures the physiologically meaningful, oscillatory patterns of HRV, and a residual variance, $\sigma^2_{\text{resid}}(t_i)$, which accounts for unstructured, moment-to-moment fluctuations or measurement noise. The complete observation model, which integrates these components, is defined by the Normal likelihood in @eq-full-likelihood.

$$
\mathrm{RRi}(t_i) \sim \mathcal{N}\left(\mu(t_i), \sigma_{\text{resid}}(t_i)\right)
$${#eq-full-likelihood}

The core of the model lies in the construction of the mean and variance components from a shared set of underlying dynamic functions. The mean trajectory, $\mu(t_i)$, is a superposition of a smoothly varying baseline trend and the synthesized structured signal itself, as shown in @eq-mean-model.

$$
\mu(t_i)
= \underbrace{\mathrm{RR_{base}}(t_i)}_{\substack{\text{Gross}\\\text{RRi Trend}}}
+
\underbrace{A(t_i) \cdot \sum_{j=1}^{J} p_j(t_i) \cdot S_j(t_i)}_{\substack{\text{Structured Variability Signal}}}
$${#eq-mean-model}

Here, $\mathrm{RR_{base}}(t_i)$ represents the gross, underlying heart period trajectory. The structured variability signal is synthesized from a set of dynamically evolving spectral oscillators, $S_j(t_i)$, which represent activity in different physiological frequency bands ($j=1,2,3$ for VLF, LF, and HF, respectively). These oscillators are weighted by time-varying proportions, $p_j(t_i)$, and their overall magnitude is controlled by a scaling amplitude, $A(t_i)$. This amplitude is deterministically calculated to ensure the signal's variance exactly matches the target structured variance, $\sigma^2_{\text{struct}}(t_i)$.

A pivotal feature is a dynamic trajectory for the total signal variability, denoted $\mathrm{SDNN}(t_i)$, which is then partitioned into its structured and residual components. This partition is controlled by a parameter, $w \in [0, 1]$, which represents the fraction of total variance attributable to the structured component, as defined in @eq-var-structure.

$$
\begin{aligned}
\sigma^2_{\text{struct}}(t_i) &= w \cdot \mathrm{SDNN}(t_i)^2 \\
\sigma^2_{\text{resid}}(t_i) &= (1-w) \cdot \mathrm{SDNN}(t_i)^2
\end{aligned}
$${#eq-var-structure}

This formulation allows the model to simultaneously learn not only how the total amount of HRV changes over time (via $\mathrm{SDNN}(t_i)$), but also how the nature of that variability evolves, that is the balance between predictable, oscillatory patterns and unpredictable noise (via $w$). This unified analysis facilitates a deeper understanding of autonomic regulation by capturing phenomena across both time and frequency domains within a single, coherent inferential framework.

### Baseline Heart Period and Total Variability Trajectories

The model posits that the primary trends in both the mean heart period and its total variability are driven by a common underlying physiological response to a stimulus. To capture this, both the $\mathrm{RR_{base}}(t_i)$ and $\mathrm{SDNN}(t_i)$ trajectories are parameterized using the same flexible double-logistic functional form.

The baseline heart period, $\mathrm{RR_{base}}(t_i)$, which quantifies the gross, underlying variations in the mean R–R interval, is defined in @eq-rri-baseline-model.

$$
\mathrm{RR_{base}}(t_i) =
\underbrace{\alpha_r}_{\substack{\text{Resting RRi}}}
-
\underbrace{\beta_r \cdot \mathcal{D}_{1}(t_i)}_{\substack{\text{Perturbation-induced}\\\text{RRi Drop}}}
+
\underbrace{c_r \beta_r \cdot \mathcal{D}_{2}(t_i)}_{\substack{\text{Post-perturbation}\\\text{RRi Recovery}}}
$${#eq-rri-baseline-model}

In this formulation, $\alpha_r$ represents the initial, stable heart period. The parameter $\beta_r$ signifies the magnitude of the decline in RRi induced by the perturbation. The fractional recovery amplitude is denoted by $c_r$, where $c_r > 1$ indicates an overshoot.

Concurrently, the total instantaneous variability, $\mathrm{SDNN}(t_i)$, follows a parallel dynamic trajectory, as defined in @eq-sdnn-model.

$$
\mathrm{SDNN}(t_i) =
\underbrace{\alpha_s}_{\substack{\text{Resting SDNN}}}
-
\underbrace{\beta_s \cdot \mathcal{D}_{1}(t_i)}_{\substack{\text{Perturbation-induced}\\\text{SDNN Drop}}}
+
\underbrace{c_s \beta_s \cdot \mathcal{D}_{2}(t_i)}_{\substack{\text{Post-perturbation}\\\text{SDNN Recovery}}}
$${#eq-sdnn-model}

Here, $\alpha_s$, $\beta_s$, and $c_s$ are analogous to their counterparts in the baseline model, representing the resting total SDNN, the magnitude of its suppression, and its fractional recovery, respectively. The dynamics for both trajectories are driven by a shared pair of logistic transition functions, $\mathcal{D}_{1}(t_i)$ and $\mathcal{D}_{2}(t_i)$, defined in @eq-logistic-components.

$$
\begin{aligned}
\mathcal{D}_{1}(t_i) &= \left(1+ e^{-\lambda (t_i - \tau)}\right)^{-1} \\
\mathcal{D}_{2}(t_i) &= \left(1+ e^{-\phi (t_i - \tau - \delta)}\right)^{-1}
\end{aligned}
$${#eq-logistic-components}

The shared timing parameters enforce a strong physiological coupling: $\tau$ is the inflection point of the initial response, with rate $\lambda$. The second transition, representing recovery, is offset by a delay $\delta$ and proceeds at a rate $\phi$. This shared structure ensures that changes in the magnitude of variability are temporally synchronized with changes in the mean heart period, while still allowing their relative magnitudes and recovery profiles to differ.

### Generative Model for the Structured Signal

The structured component of the signal is where the model's spectral properties are defined. Its construction involves three key elements: the dynamic spectral proportions, the latent spectral oscillators (whose amplitudes are governed by a Gaussian Process), and a deterministic amplitude inversion that ties them to the target variance.

#### Dynamic Frequency Band Proportions

The proportions $p_j(t_i)$ dictate how the total structured variance, $\sigma^2_{\text{struct}}(t_i)$, is allocated across the different frequency bands at each moment. The model captures the evolution of these proportions as a smooth transition between two distinct spectral states: a baseline state ($\vec\pi_{\text{base}}$) and a perturbed state ($\vec\pi_{\text{pert}}$). The transition is determined by a single master controller function, $C(t_i)$, as shown in the convex combination of @eq-pj-using-ct.

$$
\vec{p}(t_i) = (1 - C(t_i)) \cdot \vec\pi_{\text{base}} + C(t_i) \cdot \vec\pi_{\text{pert}}
$${#eq-pj-using-ct}

Here, $\vec\pi_{\text{base}}$ and $\vec\pi_{\text{pert}}$ are simplex vectors representing the characteristic spectral distributions at rest and during peak perturbation. The master controller, $C(t_i)$, is itself built from the same logistic building blocks, ensuring the spectral transition is synchronized with the primary physiological response, as defined in @eq-master-controller.

$$
C(t_i) = \mathcal{D}_{1}(t_i) \cdot \left(1 - c_c \cdot \mathcal{D}_{2}(t_i)\right)
$${#eq-master-controller}

This function naturally transitions from 0 towards 1, with the parameter $c_c \in [0, 1]$ allowing for an incomplete spectral recovery.

#### Latent Spectral Oscillators via a Gaussian Process Prior

A central task of the model is to infer the power of oscillations across a range of frequencies from the observed data. A simple approach might impose a rigid mathematical structure on the spectrum (e.g., a $1/f^b$ power law) or treat the power at each frequency as independent. However, physiological spectra are rarely so simple; they typically exhibit smooth, continuous shapes with broad peaks.

To capture this realistic structure, we employ a flexible, non-parametric Gaussian Process (GP) prior. A GP is a statistical tool that allows us to place a prior on an unknown function, enforcing the assumption that it is smooth. In this context, it formalizes the intuition that the amplitudes of nearby frequencies should be similar, allowing the model to learn the characteristic shape of the spectrum within each band directly from the data.

Each underlying oscillator, $S_j(t_i)$, which contributes to the structured variability, is modeled as a sum of simple sine and cosine waves with pre-specified frequencies ($f_{j,k}$) but unknown amplitudes ($u_{j,k}$), as shown in @eq-multi-sine-fun.

$$
S_j(t_i) = \sum_{k=1}^{K_j} \left[ u_{j,k}^{(\sin)} \sin(2 \pi f_{j,k} t_i) + u_{j,k}^{(\cos)} \cos(2 \pi f_{j,k} t_i) \right]
$${#eq-multi-sine-fun}

The key innovation is the prior placed on these amplitudes. This is a multi-step process designed for both flexibility and computational stability. First, we model the underlying log-amplitude envelope, a vector denoted $\vec{v}_j$, as a single draw from a GP. The GP is defined by a zero-mean function and a squared exponential covariance kernel ($\mathbf{K}_j$), which determines the smoothness of the spectral shape, defined in @eq-gp-kernel.

$$
\vec{v}_j \sim \mathcal{GP}(\vec{0}, \mathbf{K}_j), \quad \text{where } (\mathbf{K}_j)_{kl} = \alpha_{gp,j}^2 \exp\left(-\frac{(f_{j,k} - f_{j,l})^2}{2\rho_{gp,j}^2}\right)
$${#eq-gp-kernel}

The covariance matrix $\mathbf{K}_j$ is governed by two hyperparameters which are themselves estimated from the data: $\alpha_{gp,j}$, which correspond to the marginal standard deviation, which controls the overall magnitude or "waviness" of the spectral shape. A larger $\alpha_{gp,j}$ allows for larger peaks and deeper troughs in the spectrum. Moreover, $\rho_{gp,j}$ is the length-scale, which controls the smoothness of the function. A large $\rho_{gp,j}$ enforces a very smooth, slowly changing shape, while a small value allows for more rapid variations and narrower spectral peaks.

To ensure this complex model can be estimated efficiently, we use a non-centered parameterization. This is a computational technique that improves the performance of the sampler by reducing correlations between parameters. Instead of estimating the highly correlated vector $\vec{v}_j$ directly, the model estimates a vector of independent standard normal deviates, $\vec{z}_{gp,j}$. The target log-amplitude envelope is then deterministically constructed via the transformation $\vec{v}_j = \mathbf{L}_j \vec{z}_{gp,j}$, where $\mathbf{L}_j$ is the Cholesky factor of the covariance matrix ($\mathbf{K}_j = \mathbf{L}_j \mathbf{L}_j^\top$). This approach makes the estimation problem much simpler for the algorithm, leading to more robust and reliable inference.

#### Normalization for Identifiability

A critical step in the model's construction is to ensure that its parameters are identifiable, meaning there is a unique set of parameters that explains the data. A potential ambiguity arises because two components of the model control amplitude: the GP's marginal standard deviation ($\alpha_{gp,j}$) and the overall signal amplitude, $A(t_i)$. Without a constraint, the model could achieve the same result by increasing the GP's amplitude while decreasing $A(t_i)$, making it difficult to interpret either parameter.

To resolve this, we enforce a clear division of labor. The Gaussian Process is assigned the exclusive role of determining the relative shape of the spectrum, while $A(t_i)$ is assigned the role of setting the absolute power.

This is achieved by normalizing the amplitude envelope derived from the GP. First, let $\vec{a}_j = \exp(\vec{v}_j)$ be the vector of positive amplitudes determined by the GP. We then create a normalized scaling vector, $\vec{s}_j$, such that the expected variance of the synthesized oscillator $S_j(t_i)$ is precisely 1.

Because the final coefficients $u_{j,k}$ are generated by scaling standard normal deviates ($z_{j,k} \sim \mathcal{N}(0,1)$), the appropriate normalization must be based on the expected variance of this random process. A key property of this process is that the expected value of cross-products between independent random coefficients is zero. Consequently, the total expected variance simplifies to a sum that depends only on the diagonal elements of the pre-computed Gram matrices ($\mathbf{G}_{\sin,j}$ and $\mathbf{G}_{\cos,j}$). This procedure is formalized in @eq-normalization.

$$
\vec{s}_j = \frac{\vec{a}_j}{\sqrt{\sum_{k=1}^{K_j} a_{j,k}^2 \left( (\mathbf{G}_{\sin,j})_{kk} + (\mathbf{G}_{\cos,j})_{kk} \right)}}
$${#eq-normalization}

The final oscillator coefficients are then constructed as $u_{j,k}^{(\sin)} = z_{j,k}^{(\sin)} \cdot s_{j,k}$ and $u_{j,k}^{(\cos)} = z_{j,k}^{(\cos)} \cdot s_{j,k}$. This procedure ensures that the GP component only informs the spectral shape, normalized to have a unit expected variance, thereby removing the ambiguity and making the model's parameters clearly interpretable.

#### Deterministic Amplitude Inversion

The final step is to ensure that the synthesized structured signal, $X(t_i) = A(t_i) \sum_{j=1}^J p_j(t_i) S_j(t_i)$, has a variance equal to the target structured variance, $\sigma^2_{\text{struct}}(t_i)$, at every time point. A key feature of this model is that it does not assume the sinusoids form an orthogonal basis over the discrete, finite time domain. Instead, it computes the exact sample variance for each synthesized oscillator, $\mathrm{Var}[S_j(t_i)]$, given the current values of its coefficients.

Let $\mathbf{\Sigma}_S$ be the diagonal $3 \times 3$ matrix whose entries are these computed variances, $\mathbf{\Sigma}_S[j, j] = \mathrm{Var}[S_j(t_i)]$. Assuming independence between the bands, the variance of the weighted sum is given by the quadratic form in @eq-weighted-sum.

$$
\mathrm{Var}\left[\sum_{j=1}^J p_j(t_i) S_j(t_i)\right] = \vec{p}(t_i)^\top \mathbf{\Sigma}_S \; \vec{p}(t_i)
$${#eq-weighted-sum}

The variance of the complete structured signal is $\mathrm{Var}[X(t_i)] = A(t_i)^2 \cdot \left(\vec{p}(t_i)^\top \mathbf{\Sigma}_S \; \vec{p}(t_i)\right)$. To match our target, we set this equal to $\sigma^2_{\text{struct}}(t_i) = w \cdot \mathrm{SDNN}(t_i)^2$. Solving for $A(t_i)$ yields the critical inversion formula in @eq-amplitude-inversion.

$$
A(t_i) = \frac{\sqrt{w} \cdot \mathrm{SDNN}(t_i)}{\sqrt{\vec{p}(t_i)^\top \mathbf{\Sigma}_S \; \vec{p}(t_i)}}
$${#eq-amplitude-inversion}

This inversion is essential. It dynamically adjusts the amplitude of the synthesized spectral signal to ensure its contribution to the total variance is exactly as prescribed by the model's high-level parameters ($\mathrm{SDNN}(t_i)$ and $w$), thereby connecting the time-domain and frequency-domain components of the model.

### Model Parameterization and Priors

To ensure numerical stability and efficient sampling, all model parameters are estimated on an unconstrained real-valued scale ($\mathbb{R}$). Priors are placed on these unconstrained parameters, which are then transformed back to their constrained, physically meaningful scales within the model.

#### Timing, Rate, and Magnitude Parameters

Parameters constrained to a specific interval are parameterized on the logit scale, while positive parameters are on the log scale. For instance, the timing parameters $\tau$ and $\delta$ are mapped to the observed time interval ($t_\text{range}$) and the remaining time, respectively, using the inverse logit transformation, such that $\tau = \text{logit}^{-1}(\tau_\text{logit}) \cdot t_\text{range} + t_\text{min}$ and $\delta = \text{logit}^{-1}(\delta_\text{logit}) \cdot (t_\text{range} - \tau)$.

The positive rate parameters $\lambda$ and $\phi$ are simply log-transformed, i.e., $\lambda = \exp(\lambda_\text{log})$. The recovery coefficients for the time-domain components ($c_r,c_s$) are mapped to the interval $[0, 2]$, while the recovery parameter for spectral components ($c_c$) is mapped to the $[0, 1]$ interval, using a scaled logit function. Similarly, the fraction of structured variance, $w$, is constrained between 0 and 1 via $w = \text{logit}^{-1}(w_{\text{logit}})$.

The magnitude parameters ($\alpha_r, \beta_r, \alpha_s, \beta_s$) are scaled relative to data-derived quantities to create dimensionless parameters whose priors are easier to specify. These transformations ensure the sampler explores a valid and well-behaved parameter space.

#### Spectral and Oscillator Parameters

The spectral proportions, $\vec\pi_{\text{base}}$ and $\vec\pi_{\text{pert}}$, are mapped from the 3-dimensional simplex to 2-dimensional real vectors using the additive log-ratio (ALR) transformation. The model estimates the unconstrained vectors, which are mapped back to the simplex via the softmax function.

The GP hyperparameters are given weakly informative, regularizing priors. The marginal standard deviation is given a half-Normal prior, $\alpha_{gp} \sim \mathcal{N}^+(0, 0.5)$, favoring modest spectral amplitudes. The length-scale is given a $\text{Log-normal}(0, 0.5)$ prior, regularizing the smoothness of the spectral shape.

Finally, the non-centered parameterization is completed by placing standard Normal priors on all latent "z" variables: the GP deviates ($z_{gp}$), the sine coefficients ($z_{\sin}$), and the cosine coefficients ($z_{\cos}$). This technique is critical for efficient sampling, as it decouples the hierarchical dependencies and removes pathological posterior geometries.

## Simulation Studies

To evaluate the model's performance, we conducted a series of simulation studies. The objectives were to assess the model's ability to recover known ground-truth parameters from synthetic data and to quantitatively benchmark its reconstruction of key HRV dynamics against conventional, windowed analysis techniques. Synthetic RRi time series were generated using the model's own generative structure with pre-specified parameter values. This process provided datasets with perfectly known underlying dynamics for $\mathrm{RR}(t_i)$, $\mathrm{SDNN}(t_i)$, and $p_j(t_i)$, serving as an objective gold standard for evaluation.

We designed three distinct scenarios to probe the model's capabilities under diverse and challenging conditions. These scenarios: (1) a classic sympatho-vagal response, (2) an incomplete recovery with spectral persistence, and (3) a high noise with a stable spectrum, were chosen specifically to test the limitations inherent in standard analysis methods. Each simulated dataset was analyzed with our full Bayesian model and, for comparison, with conventional techniques: a sliding-window analysis for time-domain metrics and a Short-Time Fourier Transform (STFT) for spectral dynamics.

### Time-domain trajectory reconstruction

The capacity to accurately capture time-domain dynamics was evaluated by comparing the model's continuous estimates of $\mathrm{RR}(t_i)$ and $\mathrm{SDNN}(t_i)$ against the stepwise approximations derived from a standard sliding-window mean and standard deviation. To provide a comprehensive evaluation of reconstruction fidelity, model performance was quantified against the ground-truth trajectories using a suite of complementary metrics. The root mean squared error (RMSE) and mean absolute error (MAE) were used to measure the average magnitude of the estimation error, with MAE offering a more robust assessment against sporadic outliers. The model's bias (mean error) was calculated to identify any systematic tendency for over- or underestimation. Finally, the coefficient of determination ($R^2$) was used to assess the proportion of variance in the true signal captured by the estimate.

### Spectral dynamics reconstruction

To evaluate the reconstruction of spectral dynamics, our model's continuous estimates of the spectral proportions, $p_j(t_i)$, were compared against those derived from an STFT. For a direct comparison, the spectrogram generated by the STFT was normalized at each time step to yield proportional power within the VLF, LF, and HF bands. The performance of both methods was then assessed by comparing their estimated trajectories to the known ground-truth proportions using the same comprehensive suite of metrics (RMSE, MAE, Bias, and $R^2$). This comparative framework was designed to objectively quantify the trade-offs in temporal and frequency resolution inherent to windowed methods versus the continuous estimation provided by our model, especially in the presence of rapid transitions or low signal-to-noise ratios. As part of the model's internal validation, we also confirmed that the 95% posterior credible intervals for all generative parameters consistently contained the known ground-truth values, ensuring both inferential accuracy and appropriate uncertainty quantification.


# References

<div id="refs"></div>

# Author Contributions

Conceptualization, MC-A, CN-E; Data curation, MC-A; Investigation, MC-A; Methodology, MC-A; Supervision, CN-E; Formal analysis, MC-A; Visualization, MC-A; Writing–original draft, MC-A, CN-E; Writing–review & editing, MC-A, [...], CN-E. All authors have read and agreed to the published version of the manuscript.

# Funding

This work was funded by ANID Proyecto Fondecyt Regular Nº1250474 and by the Innovation Fund for Competitiveness of the Regional Government of Magallanes and Chilean Antarctica (BIP Code 40042452-0).

# Data availability statement

The raw data supporting the conclusions of this article will be made available by the authors without undue reservation.

# Conflicts of interests

The authors declare that the research was conducted without any commercial or financial relationships construed as as a potential conflict of interest.
